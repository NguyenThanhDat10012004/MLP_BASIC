# -*- coding: utf-8 -*-
"""A_Regression_MLP_ReLU.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D7lmKMTRps5uNGYcFmmxt7Y2aZFXOPN7

## **0. Download dataset**
**Note:** If you can't download using gdown due to limited number of downloads, please download it manually and upload it to your drive, then copy it from the drive to colab.
```python
from google.colab import drive

drive.mount('/content/drive')
!cp /path/to/dataset/on/your/drive .
```
"""

from google.colab import drive
drive.mount('/content/drive')

# https://drive.google.com/file/d/1qiUDDoYyRLBiKOoYWdFl_5WByHE8Cugu/view?usp=sharing
!gdown --id 1qiUDDoYyRLBiKOoYWdFl_5WByHE8Cugu

"""## **1. Import libraries**"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
random_state = 59
np.random.seed(random_state)
torch.manual_seed(random_state)
if torch.cuda.is_available():
    torch.cuda.manual_seed(random_state)

device

"""## **2. Read dataset**"""

dataset_path = '/content/Auto_MPG_data.csv'
dataset = pd.read_csv(dataset_path)
dataset

"""## **3. Preprocessing data**

### **3.1. X, y split**
"""

X = dataset.drop(columns='MPG').values
y = dataset['MPG'].values

"""### **3.2. Train/val/test split**"""

val_size = 0.2
test_size = 0.125
is_shuffle = True

X_train, X_val, y_train, y_val = train_test_split(
    X, y,
    test_size=val_size,
    random_state=random_state,
    shuffle=is_shuffle
)

X_train, X_test, y_train, y_test = train_test_split(
    X_train, y_train,
    test_size=test_size,
    random_state=random_state,
    shuffle=is_shuffle
)

print(f'Number of training samples: {X_train.shape[0]}')
print(f'Number of val samples: {X_val.shape[0]}')
print(f'Number of test samples: {X_test.shape[0]}')

normalizer = StandardScaler()
X_train = normalizer.fit_transform(X_train)
X_val = normalizer.transform(X_val)
X_test = normalizer.transform(X_test)

# Converting to torch tensors
X_train = torch.tensor(X_train, dtype=torch.float32)
X_val = torch.tensor(X_val, dtype=torch.float32)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32)
y_val = torch.tensor(y_val, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.float32)

"""### **3.3. Data Normalization**

## **4. Create PyTorch DataLoader**
"""

class CustomDataset(Dataset):
    def __init__(self, X, y):
        self.X = X
        self.y = y

    def __len__(self):
        return len(self.y)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

batch_size = 32

# Tạo dataset cho tập huấn luyện và tập kiểm tra
train_dataset = CustomDataset(X_train, y_train)
val_dataset = CustomDataset(X_val, y_val)

# Tạo DataLoader cho tập huấn luyện
train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True  # Xáo trộn dữ liệu trong mỗi epoch
)

# Tạo DataLoader cho tập kiểm tra
val_loader = DataLoader(
    val_dataset,
    batch_size=batch_size,
    shuffle=False  # Không xáo trộn dữ liệu khi kiểm tra
)

"""## **5. Build MLP network**"""

class MLP(nn.Module):
    def __init__(self, input_dims, hidden_dims, output_dims):
        super().__init__()
        # Định nghĩa các lớp tuyến tính (fully connected layers)
        self.linear1 = nn.Linear(input_dims, hidden_dims)
        self.linear2 = nn.Linear(hidden_dims, hidden_dims)
        self.output = nn.Linear(hidden_dims, output_dims)

    def forward(self, x):
        # Truyền dữ liệu qua các lớp và hàm kích hoạt
        x = self.linear1(x)  # Lớp tuyến tính đầu tiên
        x = F.gelu(x)  # Hàm kích hoạt ReLU
        x = self.linear2(x)  # Lớp tuyến tính thứ hai
        x = F.gelu(x)  # Hàm kích hoạt ReLU
        x = self.linear2(x)  # Lớp tuyến tính thứ hai
        x = F.gelu(x)  # Hàm kích hoạt ReLU
        out = self.output(x)  # Lớp đầu ra
        return out.squeeze(1)  # Loại bỏ chiều không cần thiết (thường là chiều 1)

# Xác định các thông số của mô hình
input_dims = X_train.shape[1]  # Số lượng đặc trưng (cột) trong dữ liệu huấn luyện
output_dims = 1  # Số lượng nhãn mục tiêu (output), trong trường hợp này là một giá trị (ví dụ: cho bài toán hồi quy)
hidden_dims = 64  # Số lượng nơ-ron trong mỗi lớp ẩn

# Tạo mô hình MLP và chuyển nó sang thiết bị (CPU hoặc GPU)
model = MLP(input_dims=input_dims, hidden_dims=hidden_dims, output_dims=output_dims).to(device)

# Đặt learning rate (lr)
lr = 1e-2  # Learning rate là 0.01 (1e-2)

# Định nghĩa hàm mất mát (loss function) cho bài toán hồi quy
criterion = nn.MSELoss()  # Sử dụng Mean Squared Error Loss cho bài toán hồi quy

# Tạo optimizer sử dụng Stochastic Gradient Descent (SGD)
optimizer = torch.optim.SGD(model.parameters(), lr=lr)  # Sử dụng SGD với learning rate lr

"""## **6. Training**"""

import torch

def r_squared(y_true, y_pred):
    # Chuyển y_true và y_pred thành tensor và đưa vào thiết bị (CPU hoặc GPU)
    y_true = torch.Tensor(y_true).to(device)
    y_pred = torch.Tensor(y_pred).to(device)

    # Tính giá trị trung bình của y_true
    mean_true = torch.mean(y_true)

    # Tính tổng bình phương của sai số tổng (Total Sum of Squares)
    ss_tot = torch.sum((y_true - mean_true) ** 2)

    # Tính tổng bình phương của sai số còn lại (Residual Sum of Squares)
    ss_res = torch.sum((y_true - y_pred) ** 2)

    # Tính R-squared (R²)
    r2 = 1 - (ss_res / ss_tot)

    return r2

epochs = 100  # Số lần lặp qua toàn bộ dữ liệu
train_losses = []  # Danh sách lưu trữ giá trị mất mát trong quá trình huấn luyện
val_losses = []  # Danh sách lưu trữ giá trị mất mát trong quá trình kiểm tra
train_r2 = []  # Danh sách lưu trữ giá trị R² trong quá trình huấn luyện
val_r2 = []  # Danh sách lưu trữ giá trị R² trong quá trình kiểm tra

for epoch in range(epochs):  # Lặp qua số lượng epochs
    # Khởi tạo các giá trị cần thiết cho mỗi epoch
    train_loss = 0.0
    train_target = []
    val_target = []
    train_predict = []
    val_predict = []

    model.train()  # Đặt mô hình vào chế độ huấn luyện

    # Huấn luyện mô hình trên dữ liệu huấn luyện
    for X_samples, y_samples in train_loader:
        X_samples = X_samples.to(device)  # Chuyển dữ liệu vào thiết bị (CPU hoặc GPU)
        y_samples = y_samples.to(device)  # Chuyển nhãn vào thiết bị

        optimizer.zero_grad()  # Đặt lại gradient về 0 trước khi tính toán mới
        outputs = model(X_samples)  # Tiến hành tính toán đầu ra từ mô hình

        # Lưu dự đoán và nhãn thật
        train_predict += outputs.tolist()
        train_target += y_samples.tolist()

        loss = criterion(outputs, y_samples)  # Tính toán loss (mất mát)
        loss.backward()  # Lan truyền ngược gradient
        optimizer.step()  # Cập nhật các tham số của mô hình

        train_loss += loss.item()  # Cộng dồn loss vào tổng train_loss

    train_loss /= len(train_loader)  # Tính loss trung bình trên tất cả các batch
    train_losses.append(train_loss)  # Lưu lại loss huấn luyện cho mỗi epoch
    train_r2.append(r_squared(train_target, train_predict))  # Lưu lại R² cho huấn luyện

    model.eval()  # Đặt mô hình vào chế độ kiểm tra (evaluation)

    val_loss = 0.0
    with torch.no_grad():  # Tắt gradient computation để tiết kiệm bộ nhớ
        for X_samples, y_samples in val_loader:  # Duyệt qua dữ liệu kiểm tra
            X_samples = X_samples.to(device)  # Chuyển dữ liệu vào thiết bị
            y_samples = y_samples.to(device)  # Chuyển nhãn vào thiết bị

            outputs = model(X_samples)  # Tiến hành tính toán đầu ra từ mô hình

            val_predict += outputs.tolist()  # Lưu dự đoán cho tập kiểm tra
            val_target += y_samples.tolist()  # Lưu nhãn thật cho tập kiểm tra

            loss = criterion(outputs, y_samples)  # Tính toán loss cho tập kiểm tra
            val_loss += loss.item()  # Cộng dồn loss vào tổng val_loss

    val_loss /= len(val_loader)  # Tính loss trung bình trên tất cả các batch
    val_losses.append(val_loss)  # Lưu lại loss kiểm tra cho mỗi epoch
    val_r2.append(r_squared(val_target, val_predict))  # Lưu lại R² cho kiểm tra

    # In kết quả sau mỗi epoch
    print(f'\nEPOCH {epoch + 1}: Training loss: {train_loss:.3f}\tValidation loss: {val_loss:.3f}')

train_r2_cpu = [r.item() for r in train_r2]  # Nếu train_r2 là tensor trên GPU
val_r2_cpu = [r.item() for r in val_r2]  # Nếu val_r2 là tensor trên GPU

# Vẽ đồ thị
fig, ax = plt.subplots(2, 2, figsize=(12, 10))

# Vẽ Training Loss
ax[0, 0].plot(train_losses, color='green')
ax[0, 0].set(xlabel='Epoch', ylabel='Loss')
ax[0, 0].set_title('Training Loss')

# Vẽ Validation Loss
ax[0, 1].plot(val_losses, color='orange')
ax[0, 1].set(xlabel='Epoch', ylabel='Loss')
ax[0, 1].set_title('Validation Loss')

# Vẽ Training R2
ax[1, 0].plot(train_r2_cpu, color='green')  # Sử dụng train_r2_cpu đã chuyển về CPU
ax[1, 0].set(xlabel='Epoch', ylabel='R2')
ax[1, 0].set_title('Training R2')

# Vẽ Validation R2
ax[1, 1].plot(val_r2_cpu, color='orange')  # Sử dụng val_r2_cpu đã chuyển về CPU
ax[1, 1].set(xlabel='Epoch', ylabel='R2')
ax[1, 1].set_title('Validation R2')

plt.show()

"""## **7. Evaluation**"""

model.eval()  # Đặt mô hình vào chế độ kiểm tra (evaluation mode)
with torch.no_grad():  # Tắt gradient computation để tiết kiệm bộ nhớ
    # Đưa dữ liệu vào thiết bị (CPU hoặc GPU)
    X_val = X_val.to(device)
    y_val = y_val.to(device)

    # Dự đoán đầu ra cho tập kiểm tra
    y_hat = model(X_val)

    # Tính toán R2 cho kết quả dự đoán
    val_set_r2 = r_squared(y_hat, y_val)

    # In kết quả R2 trên tập kiểm tra
    print('Evaluation on validation set:')
    print(f'R2: {val_set_r2:.3f}')

