# -*- coding: utf-8 -*-
"""A_NonLinear_MLP_ReLU.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P2gaOB7Yh5fRj2Fnrp7fsIgjbvvQDixe

## **0. Download dataset**
**Note:** If you can't download using gdown due to limited number of downloads, please download it manually and upload it to your drive, then copy it from the drive to colab.
```python
from google.colab import drive

drive.mount('/content/drive')
!cp /path/to/dataset/on/your/drive .
```
"""

from google.colab import drive
drive.mount('/content/drive')

# https://drive.google.com/file/d/1SqSn_8rxkk-Qvu4JLMcN_3ZFGDNa6P_V/view?usp=sharing
!gdown --id 1SqSn_8rxkk-Qvu4JLMcN_3ZFGDNa6P_V

"""## **1. Import libraries**"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
random_state = 59
np.random.seed(random_state)
torch.manual_seed(random_state)
if torch.cuda.is_available():
    torch.cuda.manual_seed(random_state)

device

"""## **2. Read dataset**"""

data_path = '/content/NonLinear_data.npy'
data = np.load(data_path, allow_pickle=True).item()
X, y = data['X'], data['labels']

"""## **3. Train/val/test split**"""

val_size = 0.2
test_size = 0.125
is_shuffle = True

X_train, X_val, y_train, y_val = train_test_split(
    X, y,
    test_size=val_size,
    random_state=random_state,
    shuffle=is_shuffle
)

X_train, X_test, y_train, y_test = train_test_split(
    X_train, y_train,
    test_size=test_size,
    random_state=random_state,
    shuffle=is_shuffle
)

print(f'Number of training samples: {X_train.shape[0]}')
print(f'Number of val samples: {X_val.shape[0]}')
print(f'Number of test samples: {X_test.shape[0]}')

normalizer = StandardScaler()

# Fit và transform dữ liệu huấn luyện
X_train = normalizer.fit_transform(X_train)

# Chỉ transform dữ liệu kiểm tra và dữ liệu kiểm tra cuối
X_val = normalizer.transform(X_val)
X_test = normalizer.transform(X_test)

# Chuyển dữ liệu đầu vào thành tensor PyTorch
X_train = torch.tensor(X_train, dtype=torch.float32)
X_val = torch.tensor(X_val, dtype=torch.float32)
X_test = torch.tensor(X_test, dtype=torch.float32)

# Chuyển nhãn (y_train, y_val, y_test) thành tensor PyTorch, với dtype là long (int64) cho nhãn
y_train = torch.tensor(y_train, dtype=torch.long)
y_val = torch.tensor(y_val, dtype=torch.long)
y_test = torch.tensor(y_test, dtype=torch.long)

"""## **4. Data Normalization**

## **5. Create PyTorch DataLoader**
"""

class CustomDataset(Dataset):
    def __init__(self, X, y):
        # Khởi tạo dữ liệu đầu vào (X) và nhãn (y)
        self.X = X
        self.y = y

    def __len__(self):
        # Trả về độ dài của dataset (số lượng mẫu)
        return len(self.y)

    def __getitem__(self, idx):
        # Trả về một mẫu dữ liệu và nhãn từ chỉ số idx
        return self.X[idx], self.y[idx]

batch_size = 32

# Tạo dataset cho tập huấn luyện, kiểm tra, và kiểm tra cuối
train_dataset = CustomDataset(X_train, y_train)
val_dataset = CustomDataset(X_val, y_val)
test_dataset = CustomDataset(X_test, y_test)

# Tạo DataLoader cho tập huấn luyện
train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True  # Xáo trộn dữ liệu trong mỗi epoch
)

# Tạo DataLoader cho tập kiểm tra
val_loader = DataLoader(
    val_dataset,
    batch_size=batch_size,
    shuffle=False  # Không xáo trộn dữ liệu khi kiểm tra
)

# Tạo DataLoader cho tập kiểm tra cuối
test_loader = DataLoader(
    test_dataset,
    batch_size=batch_size,
    shuffle=False  # Không xáo trộn dữ liệu khi kiểm tra
)

"""## **6. Build MLP network**"""

class MLP(nn.Module):
    def __init__(self, input_dims, hidden_dims, output_dims):
        super(MLP, self).__init__()
        # Định nghĩa các lớp tuyến tính và hàm kích hoạt
        self.linear1 = nn.Linear(input_dims, hidden_dims)  # Lớp tuyến tính đầu tiên
        self.output = nn.Linear(hidden_dims, output_dims)  # Lớp đầu ra
        self.relu = nn.ReLU()  # Hàm kích hoạt ReLU

    def forward(self, x):
        # Tiến hành truyền dữ liệu qua các lớp
        x = self.linear1(x)  # Lớp tuyến tính đầu tiên
        x = self.relu(x)  # Áp dụng hàm kích hoạt ReLU
        out = self.output(x)  # Lớp đầu ra
        return out.squeeze(1)  # Loại bỏ chiều không cần thiết (thường là chiều 1)

"""## **7. Training**"""

def compute_accuracy(y_hat, y_true):
    # Lấy chỉ số của lớp có xác suất cao nhất từ y_hat (dự đoán của mô hình)
    _, y_hat = torch.max(y_hat, dim=1)

    # Tính số lượng dự đoán chính xác
    correct = (y_hat == y_true).sum().item()

    # Tính độ chính xác (accuracy)
    accuracy = correct / len(y_true)

    return accuracy

# Xác định số lượng đặc trưng đầu vào và nhãn mục tiêu
input_dims = X_train.shape[1]  # Số lượng đặc trưng trong X_train (cột)
output_dims = torch.unique(y_train).shape[0]  # Số lượng nhãn mục tiêu (số lớp trong y_train)
hidden_dims = 128  # Số lượng nơ-ron trong lớp ẩn

# Tạo mô hình MLP và chuyển nó sang thiết bị (CPU hoặc GPU)
model = MLP(input_dims=input_dims, hidden_dims=hidden_dims, output_dims=output_dims).to(device)

# Đặt learning rate (lr)
lr = 1e-1  # Learning rate là 0.1 (1e-1)

# Định nghĩa hàm mất mát (loss function) cho bài toán phân loại
criterion = nn.CrossEntropyLoss()  # Sử dụng CrossEntropyLoss cho bài toán phân loại

# Tạo optimizer sử dụng Stochastic Gradient Descent (SGD)
optimizer = torch.optim.SGD(model.parameters(), lr=lr)  # Sử dụng SGD với learning rate lr

# Số lượng epochs
epochs = 100
# Danh sách lưu trữ giá trị mất mát và độ chính xác trong quá trình huấn luyện và kiểm tra
train_losses = []
val_losses = []
train_accs = []
val_accs = []

# Huấn luyện mô hình
for epoch in range(epochs):
    train_loss = 0.0
    train_target = []
    train_predict = []

    # Đặt mô hình vào chế độ huấn luyện
    model.train()

    # Duyệt qua tất cả các batch trong tập huấn luyện
    for X_samples, y_samples in train_loader:
        # Chuyển dữ liệu vào thiết bị (CPU hoặc GPU)
        X_samples = X_samples.to(device)
        y_samples = y_samples.to(device)

        # Đặt lại gradient về 0
        optimizer.zero_grad()

        # Tính toán đầu ra từ mô hình
        outputs = model(X_samples)

        # Tính toán hàm mất mát
        loss = criterion(outputs, y_samples)

        # Lan truyền ngược gradient
        loss.backward()

        # Cập nhật tham số mô hình
        optimizer.step()

        # Cộng dồn mất mát
        train_loss += loss.item()

        # Lưu dự đoán và nhãn cho tính độ chính xác
        train_predict.append(outputs.detach().cpu())  # Tách tensor ra khỏi đồ thị tính toán và chuyển về CPU
        train_target.append(y_samples.cpu())  # Chuyển nhãn về CPU

    # Tính mất mát trung bình cho huấn luyện
    train_loss /= len(train_loader)
    train_losses.append(train_loss)

    # Nối tất cả dự đoán và nhãn
    train_predict = torch.cat(train_predict)
    train_target = torch.cat(train_target)

    # Tính độ chính xác cho tập huấn luyện
    train_acc = compute_accuracy(train_predict, train_target)
    train_accs.append(train_acc)

    # Kiểm tra mô hình
    val_loss = 0.0
    val_target = []
    val_predict = []

    # Đặt mô hình vào chế độ kiểm tra (evaluation)
    model.eval()

    # Tắt gradient khi kiểm tra
    with torch.no_grad():
        for X_samples, y_samples in val_loader:
            X_samples = X_samples.to(device)
            y_samples = y_samples.to(device)

            # Dự đoán đầu ra cho tập kiểm tra
            outputs = model(X_samples)

            # Tính toán mất mát cho tập kiểm tra
            val_loss += criterion(outputs, y_samples).item()

            # Lưu dự đoán và nhãn cho kiểm tra
            val_predict.append(outputs.cpu())  # Chuyển đầu ra về CPU
            val_target.append(y_samples.cpu())  # Chuyển nhãn về CPU

    # Tính mất mát trung bình cho kiểm tra
    val_loss /= len(val_loader)
    val_losses.append(val_loss)

    # Nối tất cả dự đoán và nhãn
    val_predict = torch.cat(val_predict)
    val_target = torch.cat(val_target)

    # Tính độ chính xác cho tập kiểm tra
    val_acc = compute_accuracy(val_predict, val_target)
    val_accs.append(val_acc)

    # In kết quả sau mỗi epoch
    print(f'\nEPOCH {epoch + 1}: Training loss: {train_loss:.3f}\tValidation loss: {val_loss:.3f}')

fig, ax = plt.subplots(2, 2, figsize=(12, 10))
ax[0, 0].plot(train_losses, color='green')
ax[0, 0].set(xlabel='Epoch', ylabel='Loss')
ax[0, 0].set_title('Training Loss')

ax[0, 1].plot(val_losses, color='orange')
ax[0, 1].set(xlabel='Epoch', ylabel='Loss')
ax[0, 1].set_title('Validation Loss')

ax[1, 0].plot(train_accs, color='green')
ax[1, 0].set(xlabel='Epoch', ylabel='Accuracy')
ax[1, 0].set_title('Training Accuracy')

ax[1, 1].plot(val_accs, color='orange')
ax[1, 1].set(xlabel='Epoch', ylabel='Accuracy')
ax[1, 1].set_title('Validation Accuracy')

plt.show()

"""## **8. Evaluation**"""

val_target = []
val_predict = []
model.eval()
with torch.no_grad():
    for X_samples, y_samples in val_loader:
        X_samples = X_samples.to(device)
        y_samples = y_samples.to(device)
        outputs = model(X_samples)

        val_predict.append(outputs.cpu())
        val_target.append(y_samples.cpu())

    val_predict = torch.cat(val_predict)
    val_target = torch.cat(val_target)
    val_acc = compute_accuracy(val_predict, val_target)

    print('Evaluation on val set:')
    print(f'Accuracy: {val_acc}')